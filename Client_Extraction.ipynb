{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O65iUUBz2zKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03cc40d-a01c-434f-c802-0565d20fd878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVA3JY_V2wgE",
        "outputId": "b2d07d83-be20-464c-b870-849cdc1ad06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.phrases import Phraser\n",
        "from gensim.models import Phrases\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# mount Google Drive to access the uploaded data\n",
        "drive.mount('/content/drive')\n",
        "dir = '/content/drive/MyDrive/Panasonic/'\n",
        "os.chdir(dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(s):\n",
        "    # lowercase\n",
        "    s = s.lower()\n",
        "    \n",
        "    # remove chars that are:\n",
        "    #   ^\\w: NOT word char (not alphanumeric)\n",
        "    #   ^\\s: NOT space\n",
        "    #   ^. : NOT period\n",
        "    #   |_ : OR underscore\n",
        "    s = re.sub(r'[^.\\w\\s]|_', ' ', s)\n",
        "    \n",
        "    # remove excess spaces\n",
        "    s = re.sub(' +', ' ', s)\n",
        "    s = '\\n'.join(line.strip().rstrip('.') for line in s.split('\\n'))\n",
        "\n",
        "    return s\n",
        "\n",
        "def form_sentences(s, min_tokens=3, min_token_len=1):\n",
        "    sentences = []\n",
        "\n",
        "    # split by '.' and '\\n' to get sentences\n",
        "    s = s.replace('. ', '\\n').replace('.\\n', '\\n')\n",
        "    for sent in s.split('\\n'):\n",
        "        # tokens = space separated words in a sentence\n",
        "        # e.g., 'this is an apple' has 4 tokens\n",
        "        # Condition 1) a sentnece must have at least min_tokens (3) tokens\n",
        "        # Condition 2) each token must be of min_token_len (1)\n",
        "        tokens = [t for t in sent.split() if len(t) >= min_token_len]\n",
        "        if len(tokens) >= min_tokens:\n",
        "            sentences.append(' '.join(tokens))\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "y9FvUP6xRQga"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if path is image file, return it's extension\n",
        "# else return \"\"\n",
        "def find_ext(path):\n",
        "    lower = path.lower()\n",
        "    if path:\n",
        "        for ext in ['.png', '.jpg', '.jpeg']:\n",
        "            if lower.endswith(ext):\n",
        "                return ext\n",
        "            elif ext in lower:\n",
        "                for idx in [x.end() for x in re.finditer(ext, lower)]:\n",
        "                    if idx < len(lower) and not lower[idx].isalnum():\n",
        "                        return ext\n",
        "\n",
        "    return ''"
      ],
      "metadata": {
        "id": "BK-BOtOF8Vmd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return cleaned, tail segment of a URL\n",
        "# mainly for getting image filenames\n",
        "def get_url_tail(s):\n",
        "    ext = find_ext(s)\n",
        "    s = s.split(ext)[0].split('/')[-1]\n",
        "    if s:\n",
        "        s = s.lower()\n",
        "        s = s.replace('%20', ' ')\n",
        "        s = re.sub(r'[^.\\w\\s]|_', ' ', s)\n",
        "        s = [w for w in s.split() if w.replace('.', '').isalpha()]\n",
        "\n",
        "        return ' '.join(s)\n",
        "    \n",
        "    return ''    "
      ],
      "metadata": {
        "id": "tVQPYo-r8OjN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove words from a list that are found inside exclude_list\n",
        "def exclude_words(words, exclude_list):\n",
        "    if isinstance(words, str):\n",
        "        return ' '.join([word for word in words.split() if word.lower() not in exclude_list])\n",
        "    elif isinstance(words, list):\n",
        "        return [word for word in words if word.lower() not in exclude_list]"
      ],
      "metadata": {
        "id": "1l-aX-PhLu7t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display header\n",
        "def print_header(header):\n",
        "    print('+-'+ len(header)*'-' + '-+')\n",
        "    print(f'| {header} |')\n",
        "    print('+-'+ len(header)*'-' + '-+')"
      ],
      "metadata": {
        "id": "Wrkaw5qHJo3m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to be used for exclude_words()\n",
        "exclude = stopwords.words('english')\n",
        "common_words = ['logo', 'logos', 'hd', 'png', 'svg', 'jpg', 'jpeg', 'home', 'homepage', 'website', 'hd', 'free', 'vector', 'dot', 'gifs', 'vertical', 'language', 'download', 'white', 'black', 'color', 'rgb', 'cmyk', 'index', 'rectangle', 'transparent', 'square', 'header', 'footer', 'screenshot']\n",
        "\n",
        "data_keys = ['url_tail', 'header', 'search_value', 'alt']\n",
        "\n",
        "def json_to_clients(file, company):\n",
        "    results = {\n",
        "        'url': [],\n",
        "        'page': [],\n",
        "        'alt': [],\n",
        "        'url_tail': [],\n",
        "        'common': []\n",
        "    }\n",
        "    exclude_list = exclude + [company.lower()]\n",
        "    print_header(company)\n",
        "\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "        # each item inside the JSON file is an object containing client data\n",
        "        for logo in list(data.keys()):\n",
        "            data[logo]['url_tail'] = get_url_tail(data[logo]['url'])\n",
        "\n",
        "            # form bigrams and trigrams from client data\n",
        "            sentence_stream = []\n",
        "            for data_key in data_keys:\n",
        "                data_value = data[logo][data_key]\n",
        "                if data_value:\n",
        "                    data_value = data_value.replace('-', '_').replace(' ', '_')\n",
        "                    sentence_stream.append(exclude_words(data_value.split('_'), common_words))\n",
        "            \n",
        "            bigrams = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b'_')\n",
        "            bigram_text = [bigrams[sentence] for sentence in sentence_stream]\n",
        "            trigrams = Phrases(bigram_text, min_count=1, threshold=1)\n",
        "            trigram_text = [trigrams[sentence] for sentence in bigram_text]\n",
        "\n",
        "            # replace phrases in client data with bigrams / trigrams with delimiter \"_\"\n",
        "            # E.g., \"colruyt group is a belgian\" => \"colruyt_group is a belgian\"\n",
        "            for data_key in data_keys:\n",
        "                data_value = data[logo][data_key]\n",
        "                if data_value:\n",
        "                    data[logo][data_key] = ' '.join(trigram_text.pop(0))\n",
        "\n",
        "            # 1) find most common words\n",
        "            # 2) extend abbreviations to full forms\n",
        "            text = []\n",
        "            freq = {}\n",
        "            for key in data_keys:\n",
        "                text += data[logo][key].split()\n",
        "\n",
        "            text = [word for word in text if len(word)>=2]\n",
        "            text = exclude_words(text, exclude_list)\n",
        "\n",
        "            # create freq list of words & phrases\n",
        "            for word in sorted(list(set(text)), key=len, reverse=True):        \n",
        "                freq[word] = 0\n",
        "                for cmp in text:\n",
        "                    if word == cmp:\n",
        "                        freq[word] += 1\n",
        "                    # if current word/phrase is deemed to be a subset of a longer word/phrase, \n",
        "                    # transfer the frequency of the smaller word/phrase to the longer one.\n",
        "                    # We transfer frequency because they have the same root form.\n",
        "                    # This procedure will extend abbreviations & short forms to full forms\n",
        "                    elif len(cmp) > len(word) and word == cmp[:len(word)]:\n",
        "                        if cmp in freq:\n",
        "                            freq[cmp] += freq[word] + 1\n",
        "                        else:\n",
        "                            freq[cmp] = freq[word] + 1\n",
        "                        freq.pop(word)\n",
        "                        word = cmp\n",
        "\n",
        "            # extract the most frequent word/phrase as client name\n",
        "            # if multiple words/phrases with same frequency, concatenate them together\n",
        "            client = ''\n",
        "            if freq:\n",
        "                max_freq = max(list(freq.values()))\n",
        "                for token in freq:\n",
        "                    if freq[token] == max_freq and token.lower() not in client.lower().split():\n",
        "                        client += token + ' '\n",
        "\n",
        "            results['url'].append(data[logo]['url'])\n",
        "            results['url_tail'].append(exclude_words(data[logo]['url_tail'], exclude_list))\n",
        "            results['alt'].append(exclude_words(data[logo]['alt'], exclude_list))\n",
        "            results['page'].append(data[logo]['page'])\n",
        "            results['common'].append(client.strip())\n",
        "\n",
        "    # remove irrelevant decorators like the \"index_\" in \"index_google\" & \"index_samsung\" from client names\n",
        "    for tier in ['alt', 'url_tail', 'common']:\n",
        "        client_freq = {} \n",
        "        clients = results[tier]\n",
        "        \n",
        "        for client in clients:\n",
        "            for word in client.split():\n",
        "                if word not in client_freq:\n",
        "                    client_freq[word] = 0\n",
        "                client_freq[word] += 1\n",
        "        \n",
        "        for freq_word in list(client_freq.keys()):\n",
        "            # keep words in client_freq that appear in >50% of the dict. Otherwise pop\n",
        "            if client_freq[freq_word] < max(2, len(list(data.keys()))//2):\n",
        "                client_freq.pop(freq_word)\n",
        "        \n",
        "        # leftover words in client_freq are the duplicate decorators\n",
        "        if client_freq:\n",
        "            for freq_word in client_freq:\n",
        "                print('Duplicate:', freq_word)\n",
        "                for i in range(len(clients)):\n",
        "                    # remove decorators\n",
        "                    if freq_word in clients[i]:\n",
        "                        clients[i] = clients[i].replace(freq_word, '')\n",
        "                        clients[i] = ' '.join(clients[i].split())\n",
        "\n",
        "        results[tier] = clients\n",
        "        \n",
        "    for i in range(len(results['url'])-1, -1, -1):\n",
        "        if results['alt'][i] == results['url_tail'][i] == results['common'][i] == results['common'][i] == '':\n",
        "            results['url'] = results['url'][:i] + results['url'][i+1:]\n",
        "            results['page'] = results['page'][:i] + results['page'][i+1:]\n",
        "            results['alt'] = results['alt'][:i] + results['alt'][i+1:]\n",
        "            results['url_tail'] = results['url_tail'][:i] + results['url_tail'][i+1:]\n",
        "            results['common'] = results['common'][:i] + results['common'][i+1:]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "2GDTbr59a2Oo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_clients(results):\n",
        "    for i in range(len(results[list(results.keys())[0]])):\n",
        "        print('[1]', results['alt'][i])\n",
        "        print('[2]', results['url_tail'][i])\n",
        "        print('[3]', results['common'][i])\n",
        "        print()"
      ],
      "metadata": {
        "id": "VuOTJA2-Iw2c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'saved_data'\n",
        "all_results = results = {\n",
        "    'url': [],\n",
        "    'page': [],\n",
        "    'alt': [],\n",
        "    'url_tail': [],\n",
        "    'common': []\n",
        "}\n",
        "\n",
        "# for each company data folder found on Google Drive inside data_dir folder, \n",
        "# run json_to_clients on clients.json to extract client data\n",
        "for company in os.listdir(data_dir):\n",
        "    results = json_to_clients(os.path.join(data_dir, company, 'clients.json'), company)\n",
        "    all_results['url'].extend(results['url'])\n",
        "    all_results['page'].extend(results['page'])\n",
        "    all_results['alt'].extend(results['alt'])\n",
        "    all_results['url_tail'].extend(results['url_tail'])\n",
        "    all_results['common'].extend(results['common'])\n",
        "    print_clients(results)\n",
        "\n",
        "client_df = pd.DataFrame(all_results)\n",
        "client_df.to_csv('clients.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-Ps-p8HHmO",
        "outputId": "7904f779-b529-4922-a7dc-b1633f927159"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "| Aisle411 |\n",
            "+----------+\n",
            "+------------------+\n",
            "| Augmented Pixels |\n",
            "+------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate: partners\n",
            "Duplicate: media\n",
            "Duplicate: partners\n",
            "[1] \n",
            "[2] \n",
            "[3] lg_electronics\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] technology innovation qualcomm wireless mobile\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] innovation solutions center intel data iot pc\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] forbes\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] venturebeat\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] media\n",
            "\n",
            "[1] \n",
            "[2] \n",
            "[3] pakistan cnbc\n",
            "\n",
            "+-----------------------+\n",
            "| Artisense Corporation |\n",
            "+-----------------------+\n",
            "Duplicate: formatted\n",
            "Duplicate: formatted\n",
            "[1] siemens.png\n",
            "[2] siemens\n",
            "[3] siemens.png\n",
            "\n",
            "[1] \n",
            "[2] bombardier\n",
            "[3] bombardier\n",
            "\n",
            "[1] db.png\n",
            "[2] db\n",
            "[3] db.png\n",
            "\n",
            "[1] ecarx.png\n",
            "[2] ecarx\n",
            "[3] ecarx.png\n",
            "\n",
            "[1] \n",
            "[2] kudan\n",
            "[3] kudan\n",
            "\n",
            "[1] septentrio.png\n",
            "[2] septentrio\n",
            "[3] septentrio.png\n",
            "\n",
            "[1] _nvidia inception.png\n",
            "[2] _nvidia inception\n",
            "[3] inception.png\n",
            "\n",
            "[1] frankaemika.png\n",
            "[2] frankaemika\n",
            "[3] frankaemika.png\n",
            "\n",
            "[1] TUM.png\n",
            "[2] tum\n",
            "[3] tum\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Client Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}